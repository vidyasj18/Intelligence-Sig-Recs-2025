{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bf5f7-7c45-4955-a38a-925029257e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a question answer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf44242-5e96-49a7-8a5b-9a1899869b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717e2206-2f2b-4ec8-80d1-3ebc10df0f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (4.3.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (2.20.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (6.31.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (80.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (1.75.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.9)\n",
      "Requirement already satisfied: pillow in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.2.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (14.1.0)\n",
      "Requirement already satisfied: namex in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.2)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets tensorflow evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb841d3f-05f7-44c8-b51e-895f92216f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForQuestionAnswering, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6934816-2612-4d12-a071-b097aa53cb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dfff04a-c0e4-4c2b-b45d-f24618c3944a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3803c09e-a531-4187-986b-7b57dbf2f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                            context  \\\n",
      "0  0_0  A high court in northern India on Friday acqui...   \n",
      "1  0_1  A high court in northern India on Friday acqui...   \n",
      "2  0_2  A high court in northern India on Friday acqui...   \n",
      "3  0_3  A high court in northern India on Friday acqui...   \n",
      "4  0_4  A high court in northern India on Friday acqui...   \n",
      "\n",
      "                                            question  \\\n",
      "0          What was the amount of children murdered?   \n",
      "1               When was Pandher sentenced to death?   \n",
      "2  The court aquitted Moninder Singh Pandher of w...   \n",
      "3                                  who was acquitted   \n",
      "4                                  who was sentenced   \n",
      "\n",
      "                                             answers  \n",
      "0           {'answer_start': [260], 'text': ['19 ']}  \n",
      "1     {'answer_start': [231], 'text': ['February.']}  \n",
      "2  {'answer_start': [582], 'text': ['rape and mur...  \n",
      "3  {'answer_start': [165], 'text': ['Moninder Sin...  \n",
      "4  {'answer_start': [165], 'text': ['Moninder Sin...  \n",
      "Index(['id', 'context', 'question', 'answers'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"elsayed2002/newsqa-dataset\", split=\"train\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c4cdb3-aa23-4292-bfc4-7759e6d21713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the columns to match pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8326a90b-ee88-460e-9d71-2379ad60f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'text': 'context', 'question': 'question', 'answer': 'answers'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34d17fac-31ff-4986-9e4a-8382c92f6ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answers'] = df['answers'].apply(lambda x: {'text': [x], 'answer_start': [0]})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec47c849-38f6-4965-91ba-ba26b5437556",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset_hf = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef44d677-1c8a-428c-8b48-61ed476953aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"elsayed2002/newsqa-dataset\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359f24b-06b8-4b96-9e35-168bb5939e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd023d46-1a0e-4a8a-a64a-c1ef19befb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset has the variables or content in the form of numpy array so we need to convert it into dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d28a863-40c2-4d0b-b4fd-b2c59d1dd551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'context', 'question', 'answers'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f861e473-a156-49b6-841b-7619434dc1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handles missing data\n",
    "df = df.dropna(subset=['context', 'question', 'answers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7304f26-07f3-4095-a3cc-845b400af885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to question answer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5eea18-97a1-4c05-9944-9d1d0ed3d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply fromatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f0c681c-7973-4ce5-8bb5-88bc30fad641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'context', 'question', 'answers'], dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12a489ac-b0ad-4d9a-9136-94d0cb3bab45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'context', 'question', 'answers'],\n",
      "    num_rows: 106819\n",
      "})\n",
      "{'id': '0_0', 'context': 'A high court in northern India on Friday acquitted a wealthy businessman facing the death sentence for the killing of a teen in a case dubbed \"the house of horrors.\"Moninder Singh Pandher was sentenced to death by a lower court in February.The teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.The Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.The high court upheld Koli\\'s death sentence, Kochar said.The two were arrested two years ago after body parts packed in plastic bags were found near their home in Noida, a New Delhi suburb. Their home was later dubbed a \"house of horrors\" by the Indian media.Pandher was not named a main suspect by investigators initially, but was summoned as co-accused during the trial, Kochar said.Kochar said his client was in Australia when the teen was raped and killed.Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.', 'question': 'What was the amount of children murdered?', 'answers': {'answer_start': [0], 'text': [{'answer_start': [260], 'text': ['19 ']}]}}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_hf)\n",
    "print(dataset_hf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca84d4f-6a17-4d40-b33b-78429fb5ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b935ac2-b0d7-49a7-8427-6c95b35059c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e388794e742340779b3e4867a98c7dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\VIDYA\\.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c783d99fc3bf44d1ba25b50f6e322f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd042501a1744bebd2cd152b8928de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6d59b8a5fb418a9af93a4b6cea3d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f0abb697ea494cbe31a47316a7f95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MODEL = \"deepset/roberta-base-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "max_length = 300\n",
    "doc_stride = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c559694-548e-49c8-9974-d3a31f4e9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying tokenization function to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2ad6ec1-2743-441d-bd42-5f8a8d81b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    sample_mapping = tokenized.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = examples[\"answers\"][sample_index]\n",
    "\n",
    "        # Check if the answer exists\n",
    "        if len(answer[\"text\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        sequence_ids = tokenized.sequence_ids(i)\n",
    "\n",
    "        # find start and end token indices\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        token_end_index = len(sequence_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # If answer is outside the span\n",
    "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "\n",
    "    tokenized[\"start_positions\"] = start_positions\n",
    "    tokenized[\"end_positions\"] = end_positions\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bb4c00c8-b208-4c66-9d4a-dfdab371a953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e09678d27d4c31b1b1d4508a074467",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/106819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    prepare_train_features, \n",
    "    batched=True, \n",
    "    remove_columns=dataset.column_names  # removes 'id', 'context', etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ad178b-9d1b-45ed-8225-185795b4b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert hugging face dataset to tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cec367e-5a80-4892-a02e-124a0818c6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "tf_train_dataset = tokenized_dataset.to_tf_dataset(\n",
    "    columns=[\"input_ids\", \"attention_mask\"],\n",
    "    label_cols=[\"start_positions\", \"end_positions\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57194adb-0ab9-43f0-b6b9-8f532eb41002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pretrained model using tensorflow itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5db57bb-e5c2-4253-b78e-13e56737a271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers==4.30.2\n",
      "  Using cached transformers-4.30.2-py3-none-any.whl.metadata (113 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.2)\n",
      "  Using cached tokenizers-0.13.3.tar.gz (314 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from transformers==4.30.2) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.2) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers==4.30.2) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers==4.30.2) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers==4.30.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers==4.30.2) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers==4.30.2) (2025.4.26)\n",
      "Using cached transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [62 lines of output]\n",
      "  C:\\Users\\VIDYA\\AppData\\Local\\Temp\\pip-build-env-g0vm_xn_\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved :: Apache Software License\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\implementations\n",
      "  creating build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\n",
      "  copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\models\n",
      "  copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\decoders\n",
      "  copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\normalizers\n",
      "  copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\pre_tokenizers\n",
      "  copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\processors\n",
      "  copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-313\\tokenizers\\trainers\n",
      "  copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-313\\tokenizers\\tools\n",
      "  running build_ext\n",
      "  running build_rust\n",
      "  error: can't find Rust compiler\n",
      "  \n",
      "  If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "  \n",
      "  To update pip, run:\n",
      "  \n",
      "      pip install --upgrade pip\n",
      "  \n",
      "  and then retry package installation.\n",
      "  \n",
      "  If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.30.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b18f25e7-6022-4379-8080-94d889218c47",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtins.safe_open' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m MODEL = \u001b[33m\"\u001b[39m\u001b[33mbert-large-uncased-whole-word-masking-finetuned-squad\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mTFAutoModelForQuestionAnswering\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\auto\\auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_tf_utils.py:2964\u001b[39m, in \u001b[36mTFPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   2958\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_state_dict_in_tf2_model\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework=\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m safetensors_archive:\n\u001b[32m   2961\u001b[39m         \u001b[38;5;66;03m# Load from a PyTorch safetensors checkpoint\u001b[39;00m\n\u001b[32m   2962\u001b[39m         \u001b[38;5;66;03m# We load in TF format here because PT weights often need to be transposed, and this is much\u001b[39;00m\n\u001b[32m   2963\u001b[39m         \u001b[38;5;66;03m# faster on GPU. Loading as numpy and transposing on CPU adds several seconds to load times.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2964\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_state_dict_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2965\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m            \u001b[49m\u001b[43msafetensors_archive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtf_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# No need to build the model again\u001b[39;49;00m\n\u001b[32m   2968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2969\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2970\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_weight_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2971\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2972\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2973\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m safetensors_from_pt:\n\u001b[32m   2975\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_sharded_pytorch_safetensors_in_tf2_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_tf_pytorch_utils.py:333\u001b[39m, in \u001b[36mload_pytorch_state_dict_in_tf2_model\u001b[39m\u001b[34m(tf_model, pt_state_dict, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename, ignore_mismatched_sizes, skip_logger_warnings)\u001b[39m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Convert old format to new format if needed from a PyTorch state_dict\u001b[39;00m\n\u001b[32m    332\u001b[39m tf_keys_to_pt_keys = {}\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpt_state_dict\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgamma\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'builtins.safe_open' object is not iterable"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "MODEL = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4289e975-1e8f-4c03-8b86-78cace7bf906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switched to pytorch because the model was not able to load even after trying so hard and after installing tensorflow older version as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30844e30-975c-4471-832e-3ccb9bbcf217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (0.24.0+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (2.9.0+cpu)\n",
      "Requirement already satisfied: filelock in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a86cd23-4be4-4c1e-81a2-3931566a0e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0404b9-ec9a-4bbe-a2df-d6e390377097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "C:\\Users\\VIDYA\\AppData\\Roaming\\Python\\Python313\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "MODEL = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652e5be-fe01-4027-8651-f491bfbe0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model on some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e3a311-01e7-4156-91d7-8a6d563716bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: nlp\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "context = \"Vidya is learning NLP using Hugging Face Transformers.\"\n",
    "question = \"What is Vidya learning?\"\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "start_index = torch.argmax(outputs.start_logits)\n",
    "end_index = torch.argmax(outputs.end_logits) + 1  # include the last token\n",
    "\n",
    "answer_tokens = inputs[\"input_ids\"][0][start_index:end_index]\n",
    "answer = tokenizer.decode(answer_tokens)  # decode properly to string\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba809683-90ac-45e3-beb8-5f9cc013f1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
