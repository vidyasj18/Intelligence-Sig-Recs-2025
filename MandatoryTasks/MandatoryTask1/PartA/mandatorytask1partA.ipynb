{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "14e11cf4-60c3-40a9-b0cf-37595b4a501e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow tensorflow-hub pandas numpy nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22866dd1-1c20-4377-9b65-a5a103730b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.2.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6552c63f-532c-4c5c-afdc-220b74d2b2c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset scripts are no longer supported, but found newsqa.py",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnewsqa\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\datasets\\load.py:1397\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1392\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1393\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1394\u001b[39m )\n\u001b[32m   1396\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1397\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1413\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\datasets\\load.py:1137\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1137\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1147\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\datasets\\load.py:1036\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1031\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e1, \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m):\n\u001b[32m   1032\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m   1033\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1034\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m on the Hugging Face Hub either: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(e1).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1035\u001b[39m                 ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1038\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\datasets\\load.py:994\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m    986\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    987\u001b[39m     api.hf_hub_download(\n\u001b[32m    988\u001b[39m         repo_id=path,\n\u001b[32m    989\u001b[39m         filename=filename,\n\u001b[32m   (...)\u001b[39m\u001b[32m    992\u001b[39m         proxies=download_config.proxies,\n\u001b[32m    993\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset scripts are no longer supported, but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    995\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n\u001b[32m    996\u001b[39m     \u001b[38;5;66;03m# Use the infos from the parquet export except in some cases:\u001b[39;00m\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_dir \u001b[38;5;129;01mor\u001b[39;00m data_files \u001b[38;5;129;01mor\u001b[39;00m (revision \u001b[38;5;129;01mand\u001b[39;00m revision != \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset scripts are no longer supported, but found newsqa.py"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"newsqa\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e836f969-a7a1-4dd3-8fd5-143728402cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                            context  \\\n",
      "0  0_0  A high court in northern India on Friday acqui...   \n",
      "1  0_1  A high court in northern India on Friday acqui...   \n",
      "2  0_2  A high court in northern India on Friday acqui...   \n",
      "3  0_3  A high court in northern India on Friday acqui...   \n",
      "4  0_4  A high court in northern India on Friday acqui...   \n",
      "\n",
      "                                            question  \\\n",
      "0          What was the amount of children murdered?   \n",
      "1               When was Pandher sentenced to death?   \n",
      "2  The court aquitted Moninder Singh Pandher of w...   \n",
      "3                                  who was acquitted   \n",
      "4                                  who was sentenced   \n",
      "\n",
      "                                             answers  \n",
      "0           {'answer_start': [260], 'text': ['19 ']}  \n",
      "1     {'answer_start': [231], 'text': ['February.']}  \n",
      "2  {'answer_start': [582], 'text': ['rape and mur...  \n",
      "3  {'answer_start': [165], 'text': ['Moninder Sin...  \n",
      "4  {'answer_start': [165], 'text': ['Moninder Sin...  \n",
      "Index(['id', 'context', 'question', 'answers'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"elsayed2002/newsqa-dataset\", split=\"train\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0dbcc-220c-41fb-aa62-6b7a85395c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am extracting the context column from the dataset that too only first 500 rows so that the model works faster doesn't make alot of delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00e79388-7022-4924-b411-e9b30bfb7c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"context\"].dropna().tolist()[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58981d17-f59f-468c-94ba-955fe6c55033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A high court in northern India on Friday acquitted a wealthy businessman facing the death sentence for the killing of a teen in a case dubbed \"the house of horrors.\"Moninder Singh Pandher was sentenced to death by a lower court in February.The teen was one of 19 victims -- children and young women -- in one of the most gruesome serial killings in India in recent years.The Allahabad high court has acquitted Moninder Singh Pandher, his lawyer Sikandar B. Kochar told CNN.Pandher and his domestic employee Surinder Koli were sentenced to death in February by a lower court for the rape and murder of the 14-year-old.The high court upheld Koli\\'s death sentence, Kochar said.The two were arrested two years ago after body parts packed in plastic bags were found near their home in Noida, a New Delhi suburb. Their home was later dubbed a \"house of horrors\" by the Indian media.Pandher was not named a main suspect by investigators initially, but was summoned as co-accused during the trial, Kochar said.Kochar said his client was in Australia when the teen was raped and killed.Pandher faces trial in the remaining 18 killings and could remain in custody, the attorney said.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accc1009-d71c-432b-8f9f-bc41ce0c176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing - removing all the numbers, letters and punctuations and keeping only spaces and letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fbb1b3e7-cd9e-4245-b402-efa4b1448eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts = []\n",
    "for t in texts:\n",
    "    t = t.lower()                      # make all letters lowercase\n",
    "    t = re.sub(r'[^a-z\\s]', '', t) \n",
    "    processed_texts.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e03a120d-8d5f-4b38-8d19-142c385438f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a high court in northern india on friday acquitted a wealthy businessman facing the death sentence for the killing of a teen in a case dubbed the house of horrorsmoninder singh pandher was sentenced to death by a lower court in februarythe teen was one of  victims  children and young women  in one of the most gruesome serial killings in india in recent yearsthe allahabad high court has acquitted moninder singh pandher his lawyer sikandar b kochar told cnnpandher and his domestic employee surinder koli were sentenced to death in february by a lower court for the rape and murder of the yearoldthe high court upheld kolis death sentence kochar saidthe two were arrested two years ago after body parts packed in plastic bags were found near their home in noida a new delhi suburb their home was later dubbed a house of horrors by the indian mediapandher was not named a main suspect by investigators initially but was summoned as coaccused during the trial kochar saidkochar said his client was in australia when the teen was raped and killedpandher faces trial in the remaining  killings and could remain in custody the attorney said'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa554c-e7d8-44c2-91dd-c9f7943a1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embeddings is done by 2 methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f76ab-6bef-4a38-8dfd-a7c15d6bd3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. embedding layers using tensorflow and neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fa6e93e-e9ee-4846-b032-0e8af67f2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c0fc810-6608-43f8-aae2-0642068e6be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only 100 words are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "150b1d5d-c142-40a7-9ec4-7f5c3e45d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TfidfVectorizer(max_features=100)  \n",
    "X_layer = layer.fit_transform(processed_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64e8bd75-ecb8-4203-96a7-62533672d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76dd1080-514b-4a56-829a-558762bb6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_words = layer.get_feature_names_out()\n",
    "layer_vectors = X_layer.toarray().T  # transpose so each row is a word\n",
    "layer_df = pd.DataFrame(layer_vectors, index=layer_words)\n",
    "layer_df.to_csv(\"layer_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a99749bc-c2b8-4771-82de-b9524c23599b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0         1         2         3         4         5    \\\n",
      "about      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "according  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "after      0.061587  0.061587  0.061587  0.061587  0.061587  0.061587   \n",
      "all        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "also       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "                6         7         8         9    ...       490       491  \\\n",
      "about      0.000000  0.000000  0.000000  0.036641  ...  0.021624  0.021624   \n",
      "according  0.000000  0.000000  0.000000  0.045486  ...  0.000000  0.000000   \n",
      "after      0.061587  0.061587  0.061587  0.000000  ...  0.040546  0.040546   \n",
      "all        0.000000  0.000000  0.000000  0.135100  ...  0.053153  0.053153   \n",
      "also       0.000000  0.000000  0.000000  0.000000  ...  0.023009  0.023009   \n",
      "\n",
      "                492       493       494       495       496       497  \\\n",
      "about      0.021624  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "according  0.000000  0.115355  0.115355  0.115355  0.115355  0.115355   \n",
      "after      0.040546  0.012445  0.012445  0.012445  0.012445  0.012445   \n",
      "all        0.053153  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "also       0.023009  0.028250  0.028250  0.028250  0.028250  0.028250   \n",
      "\n",
      "                498       499  \n",
      "about      0.000000  0.000000  \n",
      "according  0.115355  0.115355  \n",
      "after      0.012445  0.012445  \n",
      "all        0.000000  0.000000  \n",
      "also       0.028250  0.028250  \n",
      "\n",
      "[5 rows x 500 columns]\n"
     ]
    }
   ],
   "source": [
    "print(layer_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa18bb-cf37-43bf-b88a-00e575a7e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e49848a2-d2af-4315-b51d-229602d24b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_df.to_csv(\"layer_word_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b09646-14f4-4424-b109-303ac71b51bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55882aec-de73-4716-b99d-f4de14ed88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VIDYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\VIDYA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b1451a95-79d5-44ae-b97b-d3ad3f2f8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"context\"].dropna().tolist()[:500]\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96ce91fe-c89f-43ac-8874-972477d2bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from gensim) (7.4.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\vidya\\appdata\\roaming\\python\\python313\\site-packages (from smart_open>=1.8.1->gensim) (1.17.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83bbb02-be22-4fe3-8da7-97006b63643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training this word2vec model - no skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ffa470c-8eab-4760-8b52-7fba2740ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(\n",
    "    sentences=tokenized_sentences,  \n",
    "    vector_size=100,                \n",
    "    window=5,                       \n",
    "    min_count=2,                    \n",
    "    workers=4,                      \n",
    "    sg=0                            \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "419a80e8-4ef6-4f51-ac0d-d18dd412cd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'the', 'to', '.', 'of', 'a', 'and', 'in', '``', \"'s\"]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.index_to_key[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee73ca-ad68-4a5a-9d62-0910b4a29f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting embeddings into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "36134c9c-3648-458e-b9a2-d91186043c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "word_embeddings = pd.DataFrame(\n",
    "    [word_vectors[word] for word in word_vectors.index_to_key],\n",
    "    index=word_vectors.index_to_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2aea966f-789f-4272-8747-29332dc8df69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>0.248380</td>\n",
       "      <td>0.321591</td>\n",
       "      <td>0.183741</td>\n",
       "      <td>0.115071</td>\n",
       "      <td>-0.158037</td>\n",
       "      <td>-0.749110</td>\n",
       "      <td>-0.009797</td>\n",
       "      <td>0.861917</td>\n",
       "      <td>-0.051418</td>\n",
       "      <td>-0.110957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.547910</td>\n",
       "      <td>-0.721111</td>\n",
       "      <td>0.567363</td>\n",
       "      <td>0.272946</td>\n",
       "      <td>0.319991</td>\n",
       "      <td>0.522129</td>\n",
       "      <td>-0.104868</td>\n",
       "      <td>-1.400068</td>\n",
       "      <td>-0.366389</td>\n",
       "      <td>0.476816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.357015</td>\n",
       "      <td>-0.180807</td>\n",
       "      <td>-0.063296</td>\n",
       "      <td>-0.127321</td>\n",
       "      <td>-0.067451</td>\n",
       "      <td>-0.414737</td>\n",
       "      <td>-0.134278</td>\n",
       "      <td>0.289228</td>\n",
       "      <td>-0.873945</td>\n",
       "      <td>0.002571</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659049</td>\n",
       "      <td>-0.662102</td>\n",
       "      <td>-0.079923</td>\n",
       "      <td>-1.101367</td>\n",
       "      <td>0.429669</td>\n",
       "      <td>-0.260931</td>\n",
       "      <td>0.475936</td>\n",
       "      <td>-1.184662</td>\n",
       "      <td>-0.576921</td>\n",
       "      <td>-0.029760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.147247</td>\n",
       "      <td>-0.017964</td>\n",
       "      <td>-0.642468</td>\n",
       "      <td>0.072882</td>\n",
       "      <td>0.648560</td>\n",
       "      <td>1.206181</td>\n",
       "      <td>0.458894</td>\n",
       "      <td>0.946469</td>\n",
       "      <td>-0.390433</td>\n",
       "      <td>1.092615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.387852</td>\n",
       "      <td>0.331764</td>\n",
       "      <td>0.601067</td>\n",
       "      <td>-0.046229</td>\n",
       "      <td>1.650734</td>\n",
       "      <td>-0.009938</td>\n",
       "      <td>0.598932</td>\n",
       "      <td>0.087084</td>\n",
       "      <td>0.820960</td>\n",
       "      <td>-0.655055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-1.065229</td>\n",
       "      <td>1.238527</td>\n",
       "      <td>-0.468381</td>\n",
       "      <td>-0.912155</td>\n",
       "      <td>0.696943</td>\n",
       "      <td>-0.518920</td>\n",
       "      <td>-0.790146</td>\n",
       "      <td>1.012803</td>\n",
       "      <td>0.154508</td>\n",
       "      <td>-0.046429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360963</td>\n",
       "      <td>-0.172021</td>\n",
       "      <td>0.290925</td>\n",
       "      <td>0.159519</td>\n",
       "      <td>1.423246</td>\n",
       "      <td>-0.494612</td>\n",
       "      <td>0.291064</td>\n",
       "      <td>-1.529008</td>\n",
       "      <td>-1.442475</td>\n",
       "      <td>-0.063852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.851401</td>\n",
       "      <td>-0.060906</td>\n",
       "      <td>-0.707035</td>\n",
       "      <td>0.096884</td>\n",
       "      <td>0.297546</td>\n",
       "      <td>0.510911</td>\n",
       "      <td>0.299325</td>\n",
       "      <td>-0.196463</td>\n",
       "      <td>0.013434</td>\n",
       "      <td>1.175358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.263013</td>\n",
       "      <td>-0.724043</td>\n",
       "      <td>0.087557</td>\n",
       "      <td>-0.479443</td>\n",
       "      <td>-0.556660</td>\n",
       "      <td>0.056970</td>\n",
       "      <td>0.293171</td>\n",
       "      <td>-0.111030</td>\n",
       "      <td>-0.298531</td>\n",
       "      <td>0.754034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       ",    0.248380  0.321591  0.183741  0.115071 -0.158037 -0.749110 -0.009797   \n",
       "the -0.357015 -0.180807 -0.063296 -0.127321 -0.067451 -0.414737 -0.134278   \n",
       "to   0.147247 -0.017964 -0.642468  0.072882  0.648560  1.206181  0.458894   \n",
       ".   -1.065229  1.238527 -0.468381 -0.912155  0.696943 -0.518920 -0.790146   \n",
       "of  -0.851401 -0.060906 -0.707035  0.096884  0.297546  0.510911  0.299325   \n",
       "\n",
       "           7         8         9   ...        90        91        92  \\\n",
       ",    0.861917 -0.051418 -0.110957  ...  0.547910 -0.721111  0.567363   \n",
       "the  0.289228 -0.873945  0.002571  ...  0.659049 -0.662102 -0.079923   \n",
       "to   0.946469 -0.390433  1.092615  ... -0.387852  0.331764  0.601067   \n",
       ".    1.012803  0.154508 -0.046429  ...  0.360963 -0.172021  0.290925   \n",
       "of  -0.196463  0.013434  1.175358  ...  0.263013 -0.724043  0.087557   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       ",    0.272946  0.319991  0.522129 -0.104868 -1.400068 -0.366389  0.476816  \n",
       "the -1.101367  0.429669 -0.260931  0.475936 -1.184662 -0.576921 -0.029760  \n",
       "to  -0.046229  1.650734 -0.009938  0.598932  0.087084  0.820960 -0.655055  \n",
       ".    0.159519  1.423246 -0.494612  0.291064 -1.529008 -1.442475 -0.063852  \n",
       "of  -0.479443 -0.556660  0.056970  0.293171 -0.111030 -0.298531  0.754034  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2e3bf-f168-4edd-882d-2b5ae26c488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving embeddings into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c441e71-33f7-4448-b295-1feb8f14c439",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings.to_csv(\"word2vec_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24806a05-f7f3-4f8c-879e-b1b848afae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training word2vec using skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "231c9c0d-96b8-4ec3-8bc2-7e84d1993668",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg = Word2Vec(\n",
    "    sentences=tokenized_sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    sg=1  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059f35f-4c61-48b0-b22b-854b73c7f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving embeddings to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "589553f6-365e-4df1-b936-4289b2884048",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors_sg = model_sg.wv\n",
    "word_embeddings_sg = pd.DataFrame(\n",
    "    [word_vectors_sg[word] for word in word_vectors_sg.index_to_key],\n",
    "    index=word_vectors_sg.index_to_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc09f838-eaae-49eb-972a-950d4d7fd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_sg.to_csv(\"word2vec_skipgram_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd510e2-2790-4dd4-a4a9-b723a9ac026a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
